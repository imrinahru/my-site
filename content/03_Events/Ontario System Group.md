---
title: 2025-11-11 — Ontario System Group
date: 2025-11-11
tags:
  - event
location:
draft: true
summary: |
  Key takeaway in 20 words.
---
## Overview

**What:** Panel/event on technology, governance, and cultural responsibility  
**When / Where:** _(add details if needed)_  
**Who:** Ontario System Group; speakers discussing ethics, policy, and the cultural dimensions of technology

## Summary

I had the opportunity to join a 

## Extended Insights

At a recent event hosted by the Ontario System Group, one idea kept resurfacing: to work meaningfully with technology today, we need to step _outside_ it. Rather than beginning with what technology can do, we should start with what we want to do—and let technology take its place within that context.

Wendell Berry captured the tension vividly when he wrote:

> “The next great division of the world will be between people who wish to live as creatures and people who wish to live as machines.”

This division is not abstract. It already shapes how we design, regulate, and talk about technology.

---

## **Technology Is Cultural, Not Just Technical**

Much of contemporary conversation about AI ethics and policy focuses on safety, rights, or compliance. Yet the deeper issue is cultural. Technology emerges from, and contributes to, specific modes of thinking and living. Ethics cannot be separated from the cultural conditions that give technology meaning.

One of the event’s central points was that **AI cannot be ethical in itself**. Only its _use_—the human choices behind it—can carry moral weight. This perspective reframes our concerns: the real question is not what AI is, but what we choose to do with it.

Our current systems are not oriented toward that. Data protection policies lack contextual awareness, and AI governance is driven primarily by commercial interests. Even public-sector tools, ideally designed for the common good, are constrained by Canada’s underfunded digital infrastructure. When public services turn to automation out of necessity, it becomes difficult to return to human-centred alternatives.

---

## **Rethinking Responsibility**

Several speakers emphasized that the traditional rights-based model is insufficient. Rights matter, but they often fail to address the distributed, collective, and long-term nature of technological impact. A shift toward **responsibility**—personal, institutional, and societal—may provide a more realistic foundation for navigating contemporary challenges.

This includes the responsibility to stay engaged, even when it is uncomfortable. Responding effectively to the pressures of automation, political polarization, or authoritarian drift requires a kind of cultural training: the ability to hold uncertainty, to act without full information, and to think beyond immediate timeframes.

---

## **Going Meta: From Users to Shapers of Technology**

One of the most compelling ideas raised at the event was the need to “go meta.” Increasing the number of people who can use digital tools is important, but insufficient. What matters more is expanding the number of people who can _change_ the tools themselves.

Designers increasingly work as maker-consumers—actively shaping the systems they use. Policy makers, too, should engage at this practical, hands-on level. If they are to guide technological development, they need the same kind of material literacy that designers and engineers cultivate. The challenge is not only expertise, but scaling that expertise so that others can meaningfully participate.

---

## **Narratives, Beliefs, and the Machine Consciousness Debate**

A notable tension emerged around the question of machine consciousness. For some, even entertaining the idea seems implausible or irrational. Yet the discomfort reveals something deeper: a clash of narratives about intelligence, agency, and the boundaries of the human.

The issue is less about whether machines “feel” and more about how differing beliefs shape our capacity to collaborate. If people are unwilling to understand the narratives that others inhabit—whether about AI, nature, or intelligence—constructive conversation becomes nearly impossible. This is not a debate about metaphysics but about operational reality.

---

## **Why Policy Alone Is Not Enough**

Despite the proliferation of governance frameworks, many participants expressed a sense of disillusionment. Policy language is often abstract, filled with terms like “should,” “may,” and “ought to,” with little clarity about implementation. Without accountability, even well-intentioned policies can fail to produce meaningful change.

It may be that technologists, policymakers, and everyday citizens are living in fundamentally different narratives:

|Technologists|Citizens|
|---|---|
|Less financially affected|More affected by non-financial harms|
|Basic technical literacy|Limited or no foundational understanding|
|Observant, exploratory|Anxious, cautious, often frustrated|

Bridging these perspectives requires more than policy. It requires philosophy, cultural analysis, long-term thinking, and new forms of public engagement.

---

## **A Moral Emergency**

Underlying these discussions is a sense of urgency. Private equity shapes the trajectory of AI in ways that resist democratization. Enforcement of policy is unclear. AI increasingly functions as a system of labor extraction, widening gaps in power and capacity.

The combination of cultural disorientation, accelerated technological change, and weak governance creates what some described as a **moral emergency**. Addressing it requires attention not only to regulation, but to the cultural narratives that guide our understanding of technology itself.

---

## **Toward a More Grounded Future**

To live well with technology, we may need to shift our orientation entirely: to see technology as one part of a broader cultural landscape, rather than the centre of it. This means staying curious, staying uncomfortable, and finding ways to reshape the tools—and the stories—that shape us.


Be outside and around technology, instead of inside, don't think from technology, think of what you want to do and contextualize it

>  “It is easy for me to imagine that the next great division of the world will be between people who wish to live as creatures and people who wish to live as machines.”  

― Wendell Berry, [Life is a Miracle: An Essay Against Modern Superstition](https://www.goodreads.com/work/quotes/74220)

Technology is cultural, current policies on ethics and protections are omitting the most important development around the culture
Technology/AI cannot be ethical, the USE of it can be ethical, and we have the agency to decide the operation of that

Data protection now is not contextual, AI governance now is highly commercial, rather than protection.

Digital charter in Canada (?) 
Protect public sector application
Underfunded services to automation, then there is no return

responsibility rather than rights, right model no longer works

Practice being uncomfortable, people need to respond quickly to totalitarian regime, and it needs practice.

Think in a different time scale

## Overview  
**What / When / Where / Who**  

## Key takeaways  


Going meta is the key, it's not having more users who can use the tool, it's how to have more users who can change the tool
How to inspire people to care the technology and our living world.

*Why it can't be done on a micro-level?*
Now designers like me are trying to be a user, a maker-consumer, then policy people should be maker-consumer as well, and try to help as many people as possible to be one. It's an operation, scaling influence problem. 

*Why you cannot accept the narrative of machine consciousness?*
People here cannot believe the feeling of machine, consciousness of machine, intelligence of machine, which comes close to whether you believe in the consciousness of a plant. It's an open window for other forms of intelligence.
People can believe in anything they want, but the division is causing operational problem. If you are not curious about the narrative of another side, you won't be able to have a constructive conversation.

Don't center around technology, also don't center around politics/policy/governance. It is far from giving a PC to a children.

I have no belief in policy making, governance, which is confirmed yet again with abstract wording and lots of should, but who is implementing it? They should do philosophy. 

Maybe the tech people and policy people (even political citizens) are living in a different narrative.

| Tech                                    | Citizen                        |
| --------------------------------------- | ------------------------------ |
| less affected financially               | more affected by other factors |
| trained to understand the basic concept | No base understanding          |
| observant                               | scared, angry                  |

## Slides & media  
- ![Slide thumbnail](./img/slide.png)  
- Recording: <https://youtu.be/…>  

## Related notes  
- https://thecon.ai/


Who's enforcing it? Unaccountable policy making. Private equity/AI cannot be democratized.

AI is labor extraction 

We have moral emergency now